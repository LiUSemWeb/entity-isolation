{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Mean Reciprocal Expected Rank\n",
    "Just a quick notebook for calculating the expected rank for each document, then using that to calculate MRER for the whole data set.\n",
    "\n",
    "The expected rank is calculated by treating the ranking task as an urn problem. Given an urn filled with $n$ marbles of which $t$ are green, the expected number of draws from the urn to get a green marble is $\\frac{n+1}{t+1}$.\n",
    "This is equivalent to assigning an i.i.d. random score to every statement and asking how far down the first true statement occurs.\n",
    "\n",
    "The Mean Reciprocol Expected Rank measures the average reciprocol of the expected rank for every document: $$\\frac{1}{\\left|\\mathcal{D}_p\\right|}\\sum^{\\left|\\mathcal{D}_p\\right|}_{i=1}{\\frac{t_i+1}{n_i+1}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Schema typing\n",
    "from typing import List, Dict, TypedDict, Tuple, NamedTuple, Iterator, TypedDict\n",
    "from functools import cached_property\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "from enum import Flag, auto\n",
    "from itertools import count\n",
    "import pickle\n",
    "\n",
    "def flatten(xss):\n",
    "    return [ x for xs in xss for x in xs ]\n",
    "\n",
    "random.seed(4423)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label(NamedTuple):\n",
    "    r: str\n",
    "    h: int\n",
    "    t: int\n",
    "    evidence: List[int]\n",
    "\n",
    "    def toDict(self):\n",
    "        return {\n",
    "            \"r\": self.r,\n",
    "            \"h\": self.h,\n",
    "            \"t\": self.t,\n",
    "            \"evidence\": self.evidence\n",
    "        }\n",
    "\n",
    "class Mention(NamedTuple):\n",
    "    name: str\n",
    "    pos: Tuple[int, int]\n",
    "    sent_id: int\n",
    "    type: str\n",
    "    text: str\n",
    "    id: int\n",
    "    global_pos: Tuple[int, int] = None\n",
    "    index: str = None\n",
    "\n",
    "    def toDict(self):\n",
    "        d = {\n",
    "            \"name\": self.name,\n",
    "            \"pos\": self.pos,\n",
    "            \"sent_id\": self.sent_id,\n",
    "            \"type\": self.type,\n",
    "            \"text\": self.text\n",
    "        }\n",
    "        if self.index:\n",
    "            d[\"index\"] = self.index\n",
    "            d[\"global_pos\"] = self.global_pos\n",
    "        return d\n",
    "\n",
    "\n",
    "# Entity = List[Mention]\n",
    "\n",
    "WordList = List[str]\n",
    "SentenceList = List[WordList]\n",
    "\n",
    "class Entity(List[Mention]):\n",
    "    doc: int\n",
    "    ent: int\n",
    "\n",
    "    @staticmethod\n",
    "    def new(ls: List, ent:int):\n",
    "        e = Entity(ls)\n",
    "        e.ent = ent\n",
    "        return e\n",
    "\n",
    "    @cached_property\n",
    "    def type(self):\n",
    "        return Counter([m.type for m in self]).most_common(1)[0][0] if len(self) > 0 else None\n",
    "        \n",
    "\n",
    "VertexSet = List[Entity]\n",
    "\n",
    "\n",
    "# class ReplaceMode(Flag):\n",
    "#     ENTITY = auto()\n",
    "#     MENTION = auto()\n",
    "#     MENTION_MARKER = auto()\n",
    "#     MASK = auto()\n",
    "#     FIXED_WIDTH = auto()\n",
    "#     FULL_WIDTH = auto()\n",
    "#     POSITION_UPDATE = auto()\n",
    "\n",
    "\n",
    "class Document:\n",
    "    vertexSet: VertexSet\n",
    "    labels: List[Label]\n",
    "    title: str\n",
    "    sents: SentenceList\n",
    "\n",
    "    def __init__(self, docnum, vertexSet, labels, title, sents):\n",
    "        self.seen = defaultdict(list)\n",
    "        self.answerset = set()\n",
    "        c = count()\n",
    "        self.labels = [Label(**l) for l in labels]\n",
    "        for lab in self.labels:\n",
    "            self.answerset.add(lab.t)\n",
    "            self.answerset.add(lab.h)\n",
    "\n",
    "        # Absolute chaos.\n",
    "        # Enumerate the entities (vertices), sort by if they're in an answer (true relation),\n",
    "        # then filter their mentions by whether tokens from those mentions have already been seen.\n",
    "        # Basically, we're trying to handle the case that a mention is assigned to multiple entities\n",
    "        # as gracefully as possible.\n",
    "        # self.vertexSet = [Entity.new([Mention(**m, text=sents[m['sent_id']][m['pos'][0]:m['pos'][1]], id=next(c)) for m in e if self.check(m)], ent=i) for i, e in \n",
    "        #                   sorted(enumerate(vertexSet), key=lambda x: (0, x[0]) if x[0] in self.answerset else (1, x[0]))]\n",
    "        \n",
    "        self.vertexSet = [None]*len(vertexSet)\n",
    "        for i, e in sorted(enumerate(vertexSet), key=lambda x: (0, x[0]) if x[0] in self.answerset else (1, x[0])):\n",
    "            en = []\n",
    "            for m in e:\n",
    "                if self.check(m):\n",
    "                    if \"text\" in m:\n",
    "                        en.append(Mention(**m, id=next(c)))\n",
    "                    else:\n",
    "                        en.append(Mention(**m, text=sents[m['sent_id']][m['pos'][0]:m['pos'][1]], id=next(c)))\n",
    "            self.vertexSet[i] = Entity.new(en, i)\n",
    "            self.vertexSet[i].doc = docnum\n",
    "\n",
    "        # for e in self.vertexSet:\n",
    "        #     e.doc = docnum\n",
    "        # self.vertexSet.sort(key=lambda x: x.ent)\n",
    "        self.title = title\n",
    "        self.sents = sents\n",
    "\n",
    "    def check(self, m: int):\n",
    "        s = m['sent_id']\n",
    "\n",
    "        covered = flatten(self.seen[s])\n",
    "        candidates = list(range(*m['pos']))\n",
    "        if any( w in covered for w in candidates ):\n",
    "            return False\n",
    "        self.seen[s].append(candidates)\n",
    "        return True\n",
    "\n",
    "    def print(self, sep=\" \"):\n",
    "        print(sep.join(' '.join(sent) for sent in self.sents))\n",
    "\n",
    "    def getMentionsInSentence(self, sentenceId: str) -> Iterator[Tuple[int, Mention]]:\n",
    "        for i, e in enumerate(self.vertexSet):\n",
    "            for m in e:\n",
    "                seen = set()\n",
    "                if m.sent_id == sentenceId:\n",
    "                    if m.pos[0] not in seen:\n",
    "                        seen.add(m.pos[0])\n",
    "                        yield i, m\n",
    "\n",
    "    def sentenceReplaceWithMarkers(self, sents=None) -> List[List[str]]:\n",
    "        map = []\n",
    "        if not sents:\n",
    "            sents = self.sents\n",
    "        for i, sent in enumerate(sents):\n",
    "            mentions = list(self.getMentionsInSentence(i))\n",
    "            # print(i, mentions)\n",
    "            row = []\n",
    "            for j, s in enumerate(sent):\n",
    "                v = [s]\n",
    "                for k, m in sorted(mentions, key=lambda x: x[1].pos[0]):\n",
    "                    if j in range(*m.pos):\n",
    "                        if j == m.pos[0]:\n",
    "                            v = [f'<M_{m.pos[0]}>']\n",
    "                        else:\n",
    "                            v = []\n",
    "                        break\n",
    "                row.extend(v)\n",
    "            map.append(row)\n",
    "        return map\n",
    "\n",
    "    def sentenceUpdate(self, apply=False):\n",
    "        map = []\n",
    "        for i, sent in enumerate(self.sentenceReplaceWithMarkers()):\n",
    "            mentions = {m.pos[0]: m for _, m in self.getMentionsInSentence(i)}\n",
    "            row = []\n",
    "            for j, s in enumerate(sent):\n",
    "                if s.startswith(\"<M_\"):\n",
    "                    m = mentions[int(s[3:-1])]\n",
    "                    if apply:\n",
    "                        m.pos[0] = len(row)\n",
    "                    row.extend(m.text)\n",
    "                    if apply:\n",
    "                        m.pos[1] = len(row)\n",
    "                    pass\n",
    "                else:\n",
    "                    row.append(s)\n",
    "            map.append(row)\n",
    "        return map\n",
    "    \n",
    "    def toDict(self):\n",
    "        return {\n",
    "            \"title\": self.title,\n",
    "            \"vertexSet\": [[m.toDict() for m in e] for e in self.vertexSet],\n",
    "            \"labels\": [lab.toDict() for lab in self.labels],\n",
    "            \"sents\": self.sents\n",
    "        }\n",
    "    \n",
    "    def toJson(self):\n",
    "        # def default(obj):\n",
    "        #     if isinstance(Mention):\n",
    "        #         return obj.toDict()\n",
    "            \n",
    "        return json.dumps(self.toDict(), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'country',\n",
       " 'desc': 'sovereign state that this item is in (not to be used for human beings)',\n",
       " 'prompt_xy': '?x is in the country of ?y.',\n",
       " 'prompt_yx': '?y is the country where ?x is located.',\n",
       " 'domain': ['ORG', 'LOC', 'MISC'],\n",
       " 'range': ['LOC'],\n",
       " 'reflexive': False,\n",
       " 'irreflexive': True,\n",
       " 'symmetric': False,\n",
       " 'antisymmetric': False,\n",
       " 'transitive': False,\n",
       " 'implied_by': [],\n",
       " 'tokens': ['?x', 'is', 'in', 'the', 'country', 'of', '?y', '.'],\n",
       " 'verb': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"data/re-docred/rel_info_domain_range.pickle\", 'rb') as pick:\n",
    "#     dev_answers = pickle.load(pick)\n",
    "\n",
    "with open(\"data/re-docred/rel_info_full.json\", 'r') as ri:\n",
    "    rel_info = json.load(ri)\n",
    "rel_info[\"P17\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRER for P17: 0.17407445029865207\n",
      "MRER for P27: 0.3272475147234804\n",
      "MRER for P131: 0.2585455308747644\n",
      "MRER for P150: 0.13219489306885063\n",
      "MRER for P161: 0.27225270606268115\n",
      "MRER for P175: 0.13170181668098732\n",
      "MRER for P527: 0.027260517926438497\n",
      "MRER for P569: 0.18630934428136633\n",
      "MRER for P570: 0.16771662674820365\n",
      "MRER for P577: 0.2502119580913329\n",
      "17.41,32.72,25.85,13.22,27.23,13.17,2.73,18.63,16.77,25.02\n",
      "Counter({'P131': 13, 'P17': 11, 'P1001': 2, 'P150': 1})\n"
     ]
    }
   ],
   "source": [
    "task_name = 're-docred'\n",
    "dset = 'dev_c'\n",
    "\n",
    "\n",
    "with open(f'data/{task_name}/{dset}.json', 'r', encoding='utf8') as docred_dev_json:\n",
    "    docred_dev = json.load(docred_dev_json)\n",
    "\n",
    "with open(f\"data/{task_name}/rel_info_full.json\", 'r') as ri:\n",
    "    rel_info = json.load(ri)\n",
    "\n",
    "# print(rel_info[\"P17\"])\n",
    "# Schema: List[{vertexSet, labels: Label, title: str, sents: SentenceList}]\n",
    "\n",
    "rers = {r:[] for r in rel_info}\n",
    "\n",
    "if 'bio' in task_name:\n",
    "    all_types = {'DiseaseOrPhenotypicFeature', 'GeneOrGeneProduct', 'ChemicalEntity', 'SequenceVariant', 'OrganismTaxon', 'CellLine'}\n",
    "else:\n",
    "    all_types = {'PER','LOC','NUM','TIME','ORG','MISC'}\n",
    "\n",
    "for i, doc in enumerate(docred_dev):\n",
    "    d = Document(docnum=i, **docred_dev[i])\n",
    "    # print(json.dumps(d.toDict(), indent=2))\n",
    "    rel_set = Counter(ans.r for ans in d.labels)\n",
    "    # Calculate the possible domain and ranges of each relation\n",
    "\n",
    "    ent_by_type = {t:[] for t in all_types}\n",
    "\n",
    "    for e in d.vertexSet:\n",
    "        if e.type:\n",
    "            ent_by_type[e.type].append(e.ent)\n",
    "\n",
    "    # print(ent_by_type)\n",
    "\n",
    "    for rel in rel_set:\n",
    "        # print(rel)\n",
    "        dom = rel_info[rel]['domain']\n",
    "        # print(dom)\n",
    "        ran = rel_info[rel]['range']\n",
    "        # print(ran)\n",
    "        _dom = set()\n",
    "        for t in dom:\n",
    "            _dom.update(ent_by_type[t])\n",
    "        _ran = set()\n",
    "        for t in ran:\n",
    "            _ran.update(ent_by_type[t])\n",
    "        # print(_dom)\n",
    "        # print(_ran)\n",
    "        t = rel_set[rel]\n",
    "        n = len(_dom)*len(_ran)\n",
    "        if(rel_info[rel]['irreflexive']):\n",
    "            n -= len(_dom&_ran)\n",
    "        # print(f\"{t=}\")\n",
    "        # print(f\"{n=}\")\n",
    "        er=(n+1)/(t+1)\n",
    "        # print(f\"ER={er}\")\n",
    "        rers[rel].append(1/er)\n",
    "    \n",
    "# print(rer)\n",
    "\n",
    "mrers = []\n",
    "\n",
    "if 'bio' in task_name:\n",
    "    relsel = ['Association','Bind','Negative_Correlation','Positive_Correlation']\n",
    "else:\n",
    "    relsel = ['P17', 'P27', 'P131', 'P150', 'P161', 'P175', 'P527', 'P569', 'P570', 'P577']\n",
    "\n",
    "for rel in relsel:\n",
    "    rer = rers[rel]\n",
    "    mrers.append(sum(rer)/len(rer))\n",
    "    print(f\"MRER for {rel}:\", sum(rer)/len(rer))\n",
    "\n",
    "print(', '.join(f'{m*100:.2f}' for m in mrers))\n",
    "\n",
    "\n",
    "# print(rel_set)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iswc24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
